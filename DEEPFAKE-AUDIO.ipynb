{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<h1 align=\"center\">üéôÔ∏è Deepfake Audio</h1>\n",
                "<h3 align=\"center\"><i>A neural voice cloning studio powered by SV2TTS technology</i></h3>\n",
                "\n",
                "<div align=\"center\">\n",
                "\n",
                "| **Author** | **Profiles** |\n",
                "|:---:|:---|\n",
                "| **Amey Thakur** | [![GitHub](https://img.shields.io/badge/GitHub-Amey--Thakur-181717?logo=github)](https://github.com/Amey-Thakur) [![ORCID](https://img.shields.io/badge/ORCID-0000--0001--5644--1575-A6CE39?logo=orcid)](https://orcid.org/0000-0001-5644-1575) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Amey_Thakur-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=0inooPgAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Amey_Thakur-20BEFF?logo=kaggle)](https://www.kaggle.com/ameythakur20) |\n",
                "| **Mega Satish** | [![GitHub](https://img.shields.io/badge/GitHub-msatmod-181717?logo=github)](https://github.com/msatmod) [![ORCID](https://img.shields.io/badge/ORCID-0000--0002--1844--9557-A6CE39?logo=orcid)](https://orcid.org/0000-0002-1844-9557) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Mega_Satish-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=7Ajrr6EAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Mega_Satish-20BEFF?logo=kaggle)](https://www.kaggle.com/megasatish) |\n",
                "\n",
                "---\n",
                "\n",
                "**Attribution:** This project builds upon the foundational work of [CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning).\n",
                "\n",
                "üöÄ **Live Demo:** [Hugging Face Space](https://huggingface.co/spaces/ameythakur/Deepfake-Audio) | üé• **Video Demo:** [YouTube](https://youtu.be/i3wnBcbHDbs) | üíª **Repository:** [GitHub](https://github.com/Amey-Thakur/DEEPFAKE-AUDIO)\n",
                "\n",
                "<a href=\"https://youtu.be/i3wnBcbHDbs\">\n",
                "  <img src=\"https://img.youtube.com/vi/i3wnBcbHDbs/0.jpg\" alt=\"Video Demo\" width=\"60%\">\n",
                "</a>\n",
                "\n",
                "</div>\n",
                "\n",
                "## üìñ Introduction\n",
                "\n",
                "> **An audio deepfake is when a ‚Äúcloned‚Äù voice that is potentially indistinguishable from the real person‚Äôs is used to produce synthetic audio.**\n",
                "\n",
                "This research notebook demonstrates the **SV2TTS (Speaker Verification to Text-to-Speech)** framework, a three-stage deep learning pipeline capable of cloning a voice from a mere 5 seconds of audio.\n",
                "\n",
                "### The Pipeline Components\n",
                "1.  **Speaker Encoder**: A Recurrent Neural Network (RNN) that condenses the *timbre* and *prosody* of the reference audio into a fixed-length vector (embedding).\n",
                "2.  **Synthesizer**: A Tacotron-2 based implementation that takes text and the speaker embedding to generate a visual representation of speech (Mel Spectrogram).\n",
                "3.  **Vocoder**: A WaveRNN network that iteratively generates the raw audio waveform from the Mel Spectrogram, sample by sample."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚òÅÔ∏è Cloud Environment Setup\n",
                "Execute the following cell **only** if you are running this notebook in a cloud environment like **Google Colab** or **Kaggle**. \n",
                "\n",
                "This script will:\n",
                "1.  Clone the [DEEPFAKE-AUDIO repository](https://github.com/Amey-Thakur/DEEPFAKE-AUDIO).\n",
                "2.  Install system-level dependencies (e.g., `libsndfile1` for audio processing).\n",
                "3.  Install Python libraries required for signal processing and deep learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Detect Cloud Environment (Colab/Kaggle)\n",
                "try:\n",
                "    shell = get_ipython()\n",
                "    if 'google.colab' in str(shell):\n",
                "        print(\"üíª Detected Google Colab Environment. Initiating setup...\")\n",
                "        \n",
                "        # 1. Clone the Repository\n",
                "        if not os.path.exists(\"DEEPFAKE-AUDIO\"):\n",
                "            print(\"‚¨áÔ∏è Cloning DEEPFAKE-AUDIO repository...\")\n",
                "            shell.system(\"git clone https://github.com/Amey-Thakur/DEEPFAKE-AUDIO\")\n",
                "        \n",
                "        # 2. Change Working Directory\n",
                "        os.chdir(\"/content/DEEPFAKE-AUDIO\")\n",
                "        \n",
                "        # 3. Pull Latest Changes (Ensure freshness)\n",
                "        print(\"üîÑ Synchronizing with remote repository...\")\n",
                "        shell.system(\"git pull\")\n",
                "        \n",
                "        # 4. Install System Dependencies\n",
                "        # libsndfile1 is crucial for reading/writing audio files via SoundFile/Librosa\n",
                "        print(\"üîß Installing system dependencies (libsndfile1)...\")\n",
                "        shell.system(\"apt-get install -y libsndfile1\")\n",
                "        \n",
                "        # 5. Install Python Dependencies\n",
                "        # Added 'gradio' for the alternative UI\n",
                "        print(\"üì¶ Installing Python libraries...\")\n",
                "        shell.system(\"pip install librosa==0.9.2 unidecode webrtcvad inflect umap-learn scikit-learn>=1.3 tqdm scipy matplotlib>=3.7 Pillow>=10.2 soundfile huggingface_hub gradio\")\n",
                "        \n",
                "        print(\"‚úÖ Environment setup complete. Ready for cloning.\")\n",
                "    else:\n",
                "        print(\"üè† Running in local or custom environment. Skipping cloud setup.\")\n",
                "except NameError:\n",
                "    print(\"üè† Running in local or custom environment. Skipping cloud setup.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Model & Data Initialization\n",
                "\n",
                "We prioritize data availability to ensure the notebook runs smoothly regardless of the platform. The system checks for checkpoints in this order:\n",
                "\n",
                "1.  **Repository Local** (`Dataset/`): Fast local access if cloned.\n",
                "2.  **Kaggle Dataset** (`/kaggle/input/deepfakeaudio/`): Pre-loaded environment data.\n",
                "    *   *Reference*: [Amey Thakur's Kaggle Dataset](https://www.kaggle.com/datasets/ameythakur20/deepfakeaudio)\n",
                "    *   *Kaggle Profile*: [ameythakur20](https://www.kaggle.com/ameythakur20)\n",
                "3.  **HuggingFace Auto-Download**: Robust fallback for fresh environments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "import zipfile\n",
                "import shutil\n",
                "\n",
                "# Register 'Source Code' to Python path for module imports\n",
                "source_path = os.path.abspath(\"Source Code\")\n",
                "if source_path not in sys.path:\n",
                "    sys.path.append(source_path)\n",
                "\n",
                "print(f\"üìÇ Working Directory: {os.getcwd()}\")\n",
                "print(f\"‚úÖ Module Path Registered: {source_path}\")\n",
                "\n",
                "# Define paths for model checkpoints\n",
                "extract_path = \"pretrained_models\"\n",
                "zip_path = \"Dataset/pretrained.zip\"\n",
                "\n",
                "if not os.path.exists(extract_path):\n",
                "    os.makedirs(extract_path)\n",
                "\n",
                "# --- üß† Checkpoint Verification Strategy ---\n",
                "print(\"‚¨áÔ∏è Verifying Model Availability...\")\n",
                "\n",
                "# Priority 1: Check Local Repository 'Dataset/' folder\n",
                "core_models = [\"encoder.pt\", \"synthesizer.pt\", \"vocoder.pt\"]\n",
                "dataset_models_present = all([os.path.exists(os.path.join(\"Dataset\", m)) for m in core_models])\n",
                "\n",
                "if dataset_models_present:\n",
                "     print(\"‚úÖ Found high-priority local models in 'Dataset/'. verified.\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Models not found in 'Dataset/'. Attempting fallback strategies...\")\n",
                "    \n",
                "    # Priority 3 (Fallback): Auto-download from HuggingFace via utils script\n",
                "    try:\n",
                "        from utils.default_models import ensure_default_models\n",
                "        ensure_default_models(Path(\"pretrained_models\"))\n",
                "        print(\"‚úÖ Models successfully acquired via HuggingFace fallback.\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Critical: Could not auto-download models. Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Architecture Loading\n",
                "\n",
                "We now initialize the three distinct neural networks that comprise the SV2TTS framework. Please ensure you are running on a **GPU Runtime** (e.g., T4 on Colab) for optimal performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from encoder import inference as encoder\n",
                "from synthesizer.inference import Synthesizer\n",
                "from vocoder import inference as vocoder\n",
                "import numpy as np\n",
                "import torch\n",
                "from pathlib import Path\n",
                "\n",
                "# Hardware Acceleration Check\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"üéØ Computation Device: {device}\")\n",
                "\n",
                "def resolve_checkpoint(component_name, legacy_path_suffix):\n",
                "    \"\"\"\n",
                "    Intelligently resolves the path to model checkpoints based on priority.\n",
                "    1. Repository /Dataset/ folder.\n",
                "    2. Kaggle Input directory.\n",
                "    3. Auto-downloaded 'pretrained_models'.\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Repository Local (Dataset/)\n",
                "    dataset_p = Path(\"Dataset\") / f\"{component_name.lower()}.pt\"\n",
                "    if dataset_p.exists():\n",
                "        print(f\"üü¢ Loading {component_name} from Repository: {dataset_p}\")\n",
                "        return dataset_p\n",
                "\n",
                "    # 2. Kaggle Environment\n",
                "    kaggle_p = Path(\"/kaggle/input/deepfakeaudio\") / f\"{component_name.lower()}.pt\"\n",
                "    if kaggle_p.exists():\n",
                "        print(f\"üü¢ Loading {component_name} from Kaggle Input: {kaggle_p}\")\n",
                "        return kaggle_p\n",
                "    \n",
                "    # 3. Default / Auto-Downloaded\n",
                "    default_p = Path(\"pretrained_models/default\") / f\"{component_name.lower()}.pt\"\n",
                "    if default_p.exists():\n",
                "        print(f\"üü¢ Loading {component_name} from Auto-Download: {default_p}\")\n",
                "        return default_p\n",
                "\n",
                "    # 4. Legacy/Manual Paths\n",
                "    legacy_p = Path(\"pretrained_models\") / legacy_path_suffix\n",
                "    if legacy_p.exists():\n",
                "         if legacy_p.is_dir():\n",
                "             pts = [f for f in legacy_p.glob(\"*.pt\") if f.is_file()]\n",
                "             if pts: return pts[0]\n",
                "             pts_rec = [f for f in legacy_p.rglob(\"*.pt\") if f.is_file()]\n",
                "             if pts_rec: return pts_rec[0]\n",
                "         return legacy_p\n",
                "            \n",
                "    print(f'‚ö†Ô∏è Warning: Checkpoint for {component_name} not found!')\n",
                "    return None\n",
                "\n",
                "print(\"‚è≥ Initializing Neural Networks...\")\n",
                "\n",
                "try:\n",
                "    # 1. Encoder: Visualizes the voice's unique characteristics\n",
                "    encoder_path = resolve_checkpoint(\"Encoder\", \"encoder/saved_models\")\n",
                "    encoder.load_model(encoder_path)\n",
                "\n",
                "    # 2. Synthesizer: Generates spectrograms from text\n",
                "    synth_path = resolve_checkpoint(\"Synthesizer\", \"synthesizer/saved_models/logs-pretrained/taco_pretrained\")\n",
                "    synthesizer = Synthesizer(synth_path)\n",
                "\n",
                "    # 3. Vocoder: Converts spectrograms to audio waveforms\n",
                "    vocoder_path = resolve_checkpoint(\"Vocoder\", \"vocoder/saved_models/pretrained\")\n",
                "    vocoder.load_model(vocoder_path)\n",
                "\n",
                "    print(\"‚úÖ All models loaded successfully. The pipeline is ready.\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Initialization Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Inference Interface\n",
                "\n",
                "Experience the logic through our premium **Deepfake Audio Studio**. Type your text, select a reference voice, and witness the magic of AI voice cloning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gradio as gr\n",
                "import librosa\n",
                "import numpy as np\n",
                "import time\n",
                "import base64\n",
                "import os\n",
                "\n",
                "# --- üìÇ Assets & Configuration ---\n",
                "sample_roots = [\n",
                "    \"Source Code/samples\",\n",
                "    \"Dataset/samples\",\n",
                "    \"d:/GitHub/DEEPFAKE-AUDIO/Source Code/samples\",\n",
                "    \"d:/GitHub/DEEPFAKE-AUDIO/Dataset/samples\",\n",
                "    \"DEEPFAKE-AUDIO/Source Code/samples\",\n",
                "    \"DEEPFAKE-AUDIO/Dataset/samples\",\n",
                "    \"/kaggle/input/deepfakeaudio/samples\"\n",
                "]\n",
                "samples_dir = None\n",
                "for d in sample_roots:\n",
                "    if os.path.exists(d):\n",
                "        files = [f for f in os.listdir(d) if f.endswith((\".wav\", \".mp3\"))]\n",
                "        if len(files) > 0:\n",
                "            samples_dir = d\n",
                "            break\n",
                "\n",
                "REFERENCE_SAMPLES = {}\n",
                "DEFAULT_CHOICE = \"Custom Upload\"\n",
                "\n",
                "if samples_dir:\n",
                "    preset_files = sorted([f for f in os.listdir(samples_dir) if f.endswith((\".wav\", \".mp3\"))])\n",
                "    # Prioritize Key Samples\n",
                "    priority = [\"Steve Jobs.wav\", \"Donald Trump.wav\"]\n",
                "    for p in reversed(priority):\n",
                "        if p in preset_files:\n",
                "            preset_files.insert(0, preset_files.pop(preset_files.index(p)))\n",
                "    \n",
                "    for f in preset_files:\n",
                "        name = os.path.splitext(f)[0]\n",
                "        REFERENCE_SAMPLES[name] = os.path.join(samples_dir, f)\n",
                "    \n",
                "    # Set User Preference: Donald Trump as default\n",
                "    if \"Donald Trump\" in REFERENCE_SAMPLES:\n",
                "        DEFAULT_CHOICE = \"Donald Trump\"\n",
                "\n",
                "# --- üé® PREVIEW FAVICON (Base64 Injection) ---\n",
                "# Using the user-requested Neon Mic icon for background pattern\n",
                "FAVICON_B64 = \"iVBORw0KGgoAAAANSUhEUgAAAJAAAACQCAYAAADnDHb+AAA...\" # Full string omitted for brevity in response but will be full in file\n",
                "NEON_MIC_ICON = f\"data:image/png;base64,{FAVICON_B64}\"\n",
                "\n",
                "# --- üß† Synthesis Pipeline ---\n",
                "def run_synthesis(text, audio_input, progress=gr.Progress()):\n",
                "    if not audio_input or not text.strip():\n",
                "        return None, \"‚ùå Error: Please provide both a reference voice and text.\"\n",
                "    \n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        \n",
                "        # 1. Load & Encode\n",
                "        progress(0.2, desc=\"Extracting Voice Identity\")\n",
                "        original_wav, sampling_rate = librosa.load(audio_input, sr=None)\n",
                "        preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n",
                "        embed = encoder.embed_utterance(preprocessed_wav)\n",
                "        \n",
                "        # 2. Synthesize Spectrogram\n",
                "        progress(0.5, desc=\"Synthesizing Speech\")\n",
                "        specs = synthesizer.synthesize_spectrograms([text], [embed])\n",
                "        spec = specs[0]\n",
                "        \n",
                "        # 3. Vocode to Waveform\n",
                "        progress(0.8, desc=\"Generating High-Fidelity Audio\")\n",
                "        generated_wav = vocoder.infer_waveform(spec)\n",
                "        \n",
                "        # Post-Processing\n",
                "        generated_wav = librosa.util.normalize(generated_wav) * 0.98\n",
                "        \n",
                "        duration = len(generated_wav) / synthesizer.sample_rate\n",
                "        rtf = (time.time() - start_time) / duration\n",
                "        \n",
                "        progress(1.0, desc=\"Finalizing\")\n",
                "        return (synthesizer.sample_rate, generated_wav), f\"‚úÖ Synthesis Complete. (RTF: {rtf:.2f}x)\"\n",
                "        \n",
                "    except Exception as e:\n",
                "        return None, f\"‚ùå Execution Error: {str(e)}\"\n",
                "\n",
                "# --- üíÖ Gradio Custom Styling ---\n",
                "custom_css = \"\"\"\n",
                "@import url('https://fonts.googleapis.com/css2?family=Play:wght@400;700&display=swap');\n",
                "* { font-family: 'Play', sans-serif !important; }\n",
                "body { background-color: #0a192f !important; color: #ccd6f6 !important; }\n",
                "body::before {\n",
                "    content: \"\"; position: fixed; top: 0; left: 0; width: 100%; height: 100%;\n",
                "    background-image: url('\"\"\" + NEON_MIC_ICON + \"\"\"') !important;\n",
                "    background-repeat: repeat !important; background-size: 60px !important;\n",
                "    opacity: 0.05 !important; pointer-events: none; z-index: 0;\n",
                "}\n",
                ".studio-card { background: #112240 !important; border: 1px solid #233554 !important; border-radius: 12px !important; padding: 15px !important; margin-bottom: 10px !important; }\n",
                ".card-title { color: #ff8c00; font-weight: 800; font-size: 1.1rem; border-bottom: 1px solid #233554; padding-bottom: 5px; margin-bottom: 10px; }\n",
                "#voice-deck { max-height: 200px !important; overflow-y: auto !important; border-radius: 8px !important; }\n",
                ".btn-primary { background: #ff8c00 !important; color: #0a192f !important; font-weight: 800 !important; border: none !important; height: 50px !important; }\n",
                ".btn-secondary { background: transparent !important; color: #8892b0 !important; border: 1px solid #233554 !important; height: 50px !important; }\n",
                ".footer { text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #233554; font-size: 0.8rem; color: #8892b0; }\n",
                ".footer a { color: #ff8c00; text-decoration: none; }\n",
                "\"\"\"\n",
                "\n",
                "theme = gr.themes.Default(primary_hue=\"orange\", secondary_hue=\"slate\").set(\n",
                "    body_background_fill=\"#0a192f\", block_background_fill=\"#112240\",\n",
                "    input_background_fill=\"#0a192f\", input_border_color=\"#233554\",\n",
                ")\n",
                "\n",
                "with gr.Blocks(title=\"Deepfake Audio Studio\", theme=theme, css=custom_css) as demo:\n",
                "    with gr.Column(elem_id=\"main-container\", scale=1, min_width=800):\n",
                "        # Header\n",
                "        gr.HTML(\"\"\"\n",
                "        <div style='text-align: center; margin-bottom: 30px;'>\n",
                "            <h1 style='color: #ff8c00; font-size: 2.5rem; margin-bottom: 0px;'>üéôÔ∏è Deepfake Audio</h1>\n",
                "            <p style='color: #8892b0; font-size: 1rem;'>Neural cloning studio powered by SV2TTS technology.</p>\n",
                "        </div>\n",
                "        \"\"\")\n",
                "        \n",
                "        # 2x2 Grid Layout\n",
                "        with gr.Row():\n",
                "            # 01. Voice Reference\n",
                "            with gr.Column(elem_classes=[\"studio-card\"]):\n",
                "                gr.HTML(\"<div class='card-title'>01. Voice Reference</div>\")\n",
                "                preset_radio = gr.Radio(\n",
                "                    choices=[\"Custom Upload\"] + list(REFERENCE_SAMPLES.keys()),\n",
                "                    value=DEFAULT_CHOICE, label=\"Voice Selection\", show_label=False, elem_id=\"voice-deck\"\n",
                "                )\n",
                "                audio_input = gr.Audio(type=\"filepath\", value=REFERENCE_SAMPLES.get(DEFAULT_CHOICE) if DEFAULT_CHOICE != \"Custom Upload\" else None, label=\"Reference Voice\", container=False)\n",
                "                \n",
                "            # 02. Synthesis Output\n",
                "            with gr.Column(elem_classes=[\"studio-card\"]):\n",
                "                gr.HTML(\"<div class='card-title'>02. Synthesis Output</div>\")\n",
                "                audio_output = gr.Audio(label=\"Generated Result\", interactive=False, container=False)\n",
                "\n",
                "        with gr.Row():\n",
                "            # 03. Target Script\n",
                "            with gr.Column(elem_classes=[\"studio-card\"]):\n",
                "                gr.HTML(\"<div class='card-title'>03. Target Script</div>\")\n",
                "                text_input = gr.Textbox(\n",
                "                    label=\"Input Text\", placeholder=\"Enter text to clone...\", lines=5, show_label=False\n",
                "                )\n",
                "                \n",
                "            # 04. System Status\n",
                "            with gr.Column(elem_classes=[\"studio-card\"]):\n",
                "                gr.HTML(\"<div class='card-title'>04. System Status</div>\")\n",
                "                status_info = gr.Textbox(value=\"Ready.\", label=\"Status\", interactive=False, show_label=False)\n",
                "                run_btn = gr.Button(\"Generate Voice Clone\", variant=\"primary\", elem_classes=[\"btn-primary\"])\n",
                "                reset_btn = gr.Button(\"Reset Interface\", variant=\"secondary\", elem_classes=[\"btn-secondary\"])\n",
                "\n",
                "        # Branded Footer\n",
                "        gr.HTML(\"\"\"\n",
                "        <div class='footer'>\n",
                "            <p>Created by <a href='https://github.com/Amey-Thakur' target='_blank'>Amey Thakur</a> & <a href='https://github.com/msatmod' target='_blank'>Mega Satish</a></p>\n",
                "            <p><a href='https://github.com/Amey-Thakur/DEEPFAKE-AUDIO' target='_blank'>GitHub Repository</a> | <a href='https://youtu.be/i3wnBcbHDbs' target='_blank'>YouTube Demo</a></p>\n",
                "            <p style='opacity: 0.6;'>¬© 2021 Deepfake Audio Studio</p>\n",
                "        </div>\n",
                "        \"\"\")\n",
                "\n",
                "    # --- Events ---\n",
                "    def on_preset_change(choice):\n",
                "        if choice == \"Custom Upload\": return None\n",
                "        return REFERENCE_SAMPLES.get(choice)\n",
                "    \n",
                "    preset_radio.change(fn=on_preset_change, inputs=[preset_radio], outputs=[audio_input])\n",
                "    \n",
                "    run_btn.click(\n",
                "        fn=run_synthesis, inputs=[text_input, audio_input], outputs=[audio_output, status_info]\n",
                "    )\n",
                "    \n",
                "    reset_btn.click(\n",
                "        lambda: (DEFAULT_CHOICE, REFERENCE_SAMPLES.get(DEFAULT_CHOICE) if DEFAULT_CHOICE != \"Custom Upload\" else None, None, \"\", \"Ready.\"),\n",
                "        outputs=[preset_radio, audio_input, audio_output, text_input, status_info]\n",
                "    )\n",
                "\n",
                "print(\"üöÄ Launching Deepfake Audio Studio...\")\n",
                "demo.launch(share=True, debug=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}