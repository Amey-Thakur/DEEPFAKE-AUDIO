{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<h1 align=\"center\">üéôÔ∏è Deepfake Audio</h1>\n",
                "<h3 align=\"center\"><i>A neural voice cloning studio powered by SV2TTS technology</i></h3>\n",
                "\n",
                "<div align=\"center\">\n",
                "\n",
                "| **Author** | **Profiles** |\n",
                "|:---:|:---|\n",
                "| **Amey Thakur** | [![GitHub](https://img.shields.io/badge/GitHub-Amey--Thakur-181717?logo=github)](https://github.com/Amey-Thakur) [![ORCID](https://img.shields.io/badge/ORCID-0000--0001--5644--1575-A6CE39?logo=orcid)](https://orcid.org/0000-0001-5644-1575) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Amey_Thakur-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=0inooPgAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Amey_Thakur-20BEFF?logo=kaggle)](https://www.kaggle.com/ameythakur20) |\n",
                "| **Mega Satish** | [![GitHub](https://img.shields.io/badge/GitHub-msatmod-181717?logo=github)](https://github.com/msatmod) [![ORCID](https://img.shields.io/badge/ORCID-0000--0002--1844--9557-A6CE39?logo=orcid)](https://orcid.org/0000-0002-1844-9557) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Mega_Satish-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=7Ajrr6EAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Mega_Satish-20BEFF?logo=kaggle)](https://www.kaggle.com/megasatish) |\n",
                "\n",
                "---\n",
                "\n",
                "**Attribution:** This project builds upon the foundational work of [CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning).\n",
                "\n",
                "üöÄ **Live Demo:** [Hugging Face Space](https://huggingface.co/spaces/ameythakur/Deepfake-Audio) | üé• **Video Demo:** [YouTube](https://youtu.be/i3wnBcbHDbs) | üíª **Repository:** [GitHub](https://github.com/Amey-Thakur/DEEPFAKE-AUDIO)\n",
                "\n",
                "<a href=\"https://youtu.be/i3wnBcbHDbs\">\n",
                "  <img src=\"https://img.youtube.com/vi/i3wnBcbHDbs/0.jpg\" alt=\"Video Demo\" width=\"60%\">\n",
                "</a>\n",
                "\n",
                "</div>\n",
                "\n",
                "## üìñ Introduction\n",
                "\n",
                "> **An audio deepfake is when a ‚Äúcloned‚Äù voice that is potentially indistinguishable from the real person‚Äôs is used to produce synthetic audio.**\n",
                "\n",
                "This research notebook demonstrates the **SV2TTS (Speaker Verification to Text-to-Speech)** framework, a three-stage deep learning pipeline capable of cloning a voice from a mere 5 seconds of audio.\n",
                "\n",
                "### üß† System Architecture\n",
                "\n",
                "The following diagram illustrates how the three components interact to produce the final audio output.\n",
                "\n",
                "```mermaid\n",
                "graph LR\n",
                "    A[üé§ Reference Audio] -->|Preprocess| B(Speaker Encoder)\n",
                "    B -->|Generates Embedding| C{Synthesizer}\n",
                "    D[üìù Input Text] --> C\n",
                "    C -->|Mel Spectrogram| E(Vocoder)\n",
                "    E -->|Waveform Synthesis| F[üîä Output Audio]\n",
                "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
                "    style F fill:#9f9,stroke:#333,stroke-width:2px\n",
                "    style B fill:#bbf,stroke:#333,stroke-width:2px\n",
                "    style C fill:#bbf,stroke:#333,stroke-width:2px\n",
                "    style E fill:#bbf,stroke:#333,stroke-width:2px\n",
                "```\n",
                "\n",
                "### The Pipeline Components\n",
                "1.  **Speaker Encoder**: A Recurrent Neural Network (RNN) that condenses the *timbre* and *prosody* of the reference audio into a fixed-length vector (embedding).\n",
                "2.  **Synthesizer**: A Tacotron-2 based implementation that takes text and the speaker embedding to generate a visual representation of speech (Mel Spectrogram).\n",
                "3.  **Vocoder**: A WaveRNN network that iteratively generates the raw audio waveform from the Mel Spectrogram, sample by sample."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚òÅÔ∏è Cloud Environment Setup\n",
                "Execute the following cell **only** if you are running this notebook in a cloud environment like **Google Colab** or **Kaggle**. \n",
                "\n",
                "This script will:\n",
                "1.  Clone the [DEEPFAKE-AUDIO repository](https://github.com/Amey-Thakur/DEEPFAKE-AUDIO).\n",
                "2.  Install system-level dependencies (e.g., `libsndfile1` for audio processing).\n",
                "3.  Install Python libraries required for signal processing and deep learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Detect Cloud Environment (Colab/Kaggle)\n",
                "try:\n",
                "    shell = get_ipython()\n",
                "    if 'google.colab' in str(shell):\n",
                "        print(\"üíª Detected Google Colab Environment. Initiating setup...\")\n",
                "        \n",
                "        # 1. Clone the Repository\n",
                "        if not os.path.exists(\"DEEPFAKE-AUDIO\"):\n",
                "            print(\"‚¨áÔ∏è Cloning DEEPFAKE-AUDIO repository...\")\n",
                "            shell.system(\"git clone https://github.com/Amey-Thakur/DEEPFAKE-AUDIO\")\n",
                "        \n",
                "        # 2. Change Working Directory\n",
                "        os.chdir(\"/content/DEEPFAKE-AUDIO\")\n",
                "        \n",
                "        # 3. Pull Latest Changes (Ensure freshness)\n",
                "        print(\"üîÑ Synchronizing with remote repository...\")\n",
                "        shell.system(\"git pull\")\n",
                "        \n",
                "        # 4. Install System Dependencies\n",
                "        # libsndfile1 is crucial for reading/writing audio files via SoundFile/Librosa\n",
                "        print(\"üîß Installing system dependencies (libsndfile1)...\")\n",
                "        shell.system(\"apt-get install -y libsndfile1\")\n",
                "        \n",
                "        # 5. Install Python Dependencies\n",
                "        # Added 'gradio' for the alternative UI\n",
                "        print(\"üì¶ Installing Python libraries...\")\n",
                "        shell.system(\"pip install librosa==0.9.2 unidecode webrtcvad inflect umap-learn scikit-learn>=1.3 tqdm scipy matplotlib>=3.7 Pillow>=10.2 soundfile huggingface_hub gradio\")\n",
                "        \n",
                "        print(\"‚úÖ Environment setup complete. Ready for cloning.\")\n",
                "    else:\n",
                "        print(\"üè† Running in local or custom environment. Skipping cloud setup.\")\n",
                "except NameError:\n",
                "    print(\"üè† Running in local or custom environment. Skipping cloud setup.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Model & Data Initialization\n",
                "\n",
                "We prioritize data availability to ensure the notebook runs smoothly regardless of the platform. The system checks for checkpoints in this order:\n",
                "\n",
                "1.  **Repository Local** (`Dataset/`): Fast local access if cloned.\n",
                "2.  **Kaggle Dataset** (`/kaggle/input/deepfakeaudio/`): Pre-loaded environment data.\n",
                "    *   *Reference*: [Amey Thakur's Kaggle Dataset](https://www.kaggle.com/datasets/ameythakur20/deepfakeaudio)\n",
                "    *   *Kaggle Profile*: [ameythakur20](https://www.kaggle.com/ameythakur20)\n",
                "3.  **HuggingFace Auto-Download**: Robust fallback for fresh environments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "import zipfile\n",
                "import shutil\n",
                "\n",
                "# Register 'Source Code' to Python path for module imports\n",
                "source_path = os.path.abspath(\"Source Code\")\n",
                "if source_path not in sys.path:\n",
                "    sys.path.append(source_path)\n",
                "\n",
                "print(f\"üìÇ Working Directory: {os.getcwd()}\")\n",
                "print(f\"‚úÖ Module Path Registered: {source_path}\")\n",
                "\n",
                "# Define paths for model checkpoints\n",
                "extract_path = \"pretrained_models\"\n",
                "zip_path = \"Dataset/pretrained.zip\"\n",
                "\n",
                "if not os.path.exists(extract_path):\n",
                "    os.makedirs(extract_path)\n",
                "\n",
                "# --- üß† Checkpoint Verification Strategy ---\n",
                "print(\"‚¨áÔ∏è Verifying Model Availability...\")\n",
                "\n",
                "# Priority 1: Check Local Repository 'Dataset/' folder\n",
                "core_models = [\"encoder.pt\", \"synthesizer.pt\", \"vocoder.pt\"]\n",
                "dataset_models_present = all([os.path.exists(os.path.join(\"Dataset\", m)) for m in core_models])\n",
                "\n",
                "if dataset_models_present:\n",
                "     print(\"‚úÖ Found high-priority local models in 'Dataset/'. verified.\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Models not found in 'Dataset/'. Attempting fallback strategies...\")\n",
                "    \n",
                "    # Priority 3 (Fallback): Auto-download from HuggingFace via utils script\n",
                "    try:\n",
                "        from utils.default_models import ensure_default_models\n",
                "        ensure_default_models(Path(\"pretrained_models\"))\n",
                "        print(\"‚úÖ Models successfully acquired via HuggingFace fallback.\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Critical: Could not auto-download models. Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Architecture Loading\n",
                "\n",
                "We now initialize the three distinct neural networks that comprise the SV2TTS framework. Please ensure you are running on a **GPU Runtime** (e.g., T4 on Colab) for optimal performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from encoder import inference as encoder\n",
                "from synthesizer.inference import Synthesizer\n",
                "from vocoder import inference as vocoder\n",
                "import numpy as np\n",
                "import torch\n",
                "from pathlib import Path\n",
                "\n",
                "# Hardware Acceleration Check\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"üéØ Computation Device: {device}\")\n",
                "\n",
                "def resolve_checkpoint(component_name, legacy_path_suffix):\n",
                "    \"\"\"\n",
                "    Intelligently resolves the path to model checkpoints based on priority.\n",
                "    1. Repository /Dataset/ folder.\n",
                "    2. Kaggle Input directory.\n",
                "    3. Auto-downloaded 'pretrained_models'.\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Repository Local (Dataset/)\n",
                "    dataset_p = Path(\"Dataset\") / f\"{component_name.lower()}.pt\"\n",
                "    if dataset_p.exists():\n",
                "        print(f\"üü¢ Loading {component_name} from Repository: {dataset_p}\")\n",
                "        return dataset_p\n",
                "\n",
                "    # 2. Kaggle Environment\n",
                "    kaggle_p = Path(\"/kaggle/input/deepfakeaudio\") / f\"{component_name.lower()}.pt\"\n",
                "    if kaggle_p.exists():\n",
                "        print(f\"üü¢ Loading {component_name} from Kaggle Input: {kaggle_p}\")\n",
                "        return kaggle_p\n",
                "    \n",
                "    # 3. Default / Auto-Downloaded\n",
                "    default_p = Path(\"pretrained_models/default\") / f\"{component_name.lower()}.pt\"\n",
                "    if default_p.exists():\n",
                "        print(f\"üü¢ Loading {component_name} from Auto-Download: {default_p}\")\n",
                "        return default_p\n",
                "\n",
                "    # 4. Legacy/Manual Paths\n",
                "    legacy_p = Path(\"pretrained_models\") / legacy_path_suffix\n",
                "    if legacy_p.exists():\n",
                "         if legacy_p.is_dir():\n",
                "             pts = [f for f in legacy_p.glob(\"*.pt\") if f.is_file()]\n",
                "             if pts: return pts[0]\n",
                "             pts_rec = [f for f in legacy_p.rglob(\"*.pt\") if f.is_file()]\n",
                "             if pts_rec: return pts_rec[0]\n",
                "         return legacy_p\n",
                "            \n",
                "    print(f'‚ö†Ô∏è Warning: Checkpoint for {component_name} not found!')\n",
                "    return None\n",
                "\n",
                "print(\"‚è≥ Initializing Neural Networks...\")\n",
                "\n",
                "try:\n",
                "    # 1. Encoder: Visualizes the voice's unique characteristics\n",
                "    encoder_path = resolve_checkpoint(\"Encoder\", \"encoder/saved_models\")\n",
                "    encoder.load_model(encoder_path)\n",
                "\n",
                "    # 2. Synthesizer: Generates spectrograms from text\n",
                "    synth_path = resolve_checkpoint(\"Synthesizer\", \"synthesizer/saved_models/logs-pretrained/taco_pretrained\")\n",
                "    synthesizer = Synthesizer(synth_path)\n",
                "\n",
                "    # 3. Vocoder: Converts spectrograms to audio waveforms\n",
                "    vocoder_path = resolve_checkpoint(\"Vocoder\", \"vocoder/saved_models/pretrained\")\n",
                "    vocoder.load_model(vocoder_path)\n",
                "\n",
                "    print(\"‚úÖ All models loaded successfully. The pipeline is ready.\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Initialization Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Inference Interface\n",
                "\n",
                "Select your **Input Method** below to begin cloning.\n",
                "\n",
                "*   **Option A: Classic Widget UI (Default)**: Simple, lightweight, and reliable.\n",
                "*   **Option B: Modern Gradio UI**: Advanced interface with waveform visualization and easy downloading.\n",
                "\n",
                "Run either cell below to start."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- OPTION A: CLASSIC WIDGET UI ---\n",
                "\n",
                "import ipywidgets as widgets\n",
                "from IPython.display import display, Javascript, Audio, HTML\n",
                "from google.colab import output\n",
                "from base64 import b64decode, b64encode\n",
                "import io\n",
                "import librosa\n",
                "import librosa.display\n",
                "import soundfile as sf\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "# --- JAVASCRIPT: Audio Recording Logic ---\n",
                "# This script injects JS into the browser to capture microphone input.\n",
                "RECORD = \"\"\"\n",
                "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
                "const b2text = blob => new Promise(resolve => {\n",
                "  const reader = new FileReader()\n",
                "  reader.onloadend = e => resolve(e.srcElement.result)\n",
                "  reader.readAsDataURL(blob)\n",
                "})\n",
                "var record = time => new Promise(async resolve => {\n",
                "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
                "  recorder = new MediaRecorder(stream)\n",
                "  chunks = []\n",
                "  recorder.ondataavailable = e => chunks.push(e.data)\n",
                "  recorder.start()\n",
                "  await sleep(time)\n",
                "  recorder.onstop = async ()=>{\n",
                "    blob = new Blob(chunks)\n",
                "    text = await b2text(blob)\n",
                "    resolve(text)\n",
                "  }\n",
                "  recorder.stop()\n",
                "})\"\"\"\n",
                "\n",
                "def record_audio(sec=10):\n",
                "    \"\"\"Invokes the JS recorder and saves the result to 'recording.wav'.\"\"\"\n",
                "    print(\"üî¥ Recording active for %d seconds...\" % sec)\n",
                "    display(Javascript(RECORD))\n",
                "    s = output.eval_js('record(%d)' % (sec*1000))\n",
                "    print(\"‚úÖ Recording saved.\")\n",
                "    binary = b64decode(s.split(',')[1])\n",
                "    with open('recording.wav', 'wb') as f:\n",
                "        f.write(binary)\n",
                "    return 'recording.wav'\n",
                "\n",
                "# --- VISUALIZATION FUNCTION ---\n",
                "def visualize_results(original_wav, generated_wav, spec, embed, title=\"Cloning Analysis\"):\n",
                "    fig, axes = plt.subplots(3, 1, figsize=(10, 12))\n",
                "    \n",
                "    # 1. Waveform Comparison\n",
                "    axes[0].set_title(\"Input Voice vs. Cloned Voice (Waveform)\")\n",
                "    librosa.display.waveshow(original_wav, alpha=0.5, ax=axes[0], label=\"Original\")\n",
                "    librosa.display.waveshow(generated_wav, alpha=0.5, ax=axes[0], label=\"Cloned\", color='r')\n",
                "    axes[0].legend()\n",
                "    \n",
                "    # 2. Spectrogram\n",
                "    axes[1].set_title(\"Generated Mel Spectrogram\")\n",
                "    im = axes[1].imshow(spec, aspect=\"auto\", origin=\"lower\", interpolation='none')\n",
                "    fig.colorbar(im, ax=axes[1])\n",
                "    \n",
                "    # 3. Embedding Heatmap\n",
                "    axes[2].set_title(\"Speaker Embedding (256-D Latent Representation)\")\n",
                "    if len(embed) == 256:\n",
                "        # Reshape to 16x16 for a square heatmap visualization\n",
                "        axes[2].imshow(embed.reshape(16, 16), aspect='auto', cmap='viridis')\n",
                "    else:\n",
                "        axes[2].plot(embed) \n",
                "        \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# --- WIDGETS: User Interface Construction ---\n",
                "\n",
                "print(\"Select Input Method:\")\n",
                "tab = widgets.Tab()\n",
                "\n",
                "# Tab 1: Presets (Celebrity Samples)\n",
                "sample_roots = [\n",
                "    \"Source Code/samples\",\n",
                "    \"Dataset/samples\",\n",
                "    \"/kaggle/input/deepfakeaudio/samples\"\n",
                "]\n",
                "samples_dir = \"Source Code/samples\" # Default fallback\n",
                "for d in sample_roots:\n",
                "    if os.path.exists(d) and len(os.listdir(d)) > 0:\n",
                "        samples_dir = d\n",
                "        print(f\"üìÇ Loading Reference Samples from: {d}\")\n",
                "        break\n",
                "\n",
                "# Filter for audio files\n",
                "preset_files = [f for f in os.listdir(samples_dir) if f.endswith(\".wav\") or f.endswith(\".mp3\")]\n",
                "preset_files.sort()\n",
                "\n",
                "# Prioritize Key Samples\n",
                "priority_samples = [\"Steve Jobs.wav\", \"Donald Trump.wav\"]\n",
                "for sample in reversed(priority_samples):\n",
                "    if sample in preset_files:\n",
                "        preset_files.insert(0, preset_files.pop(preset_files.index(sample)))\n",
                "\n",
                "dropdown = widgets.Dropdown(options=preset_files, description='Preset:')\n",
                "tab1 = widgets.VBox([dropdown])\n",
                "\n",
                "# Tab 2: File Upload\n",
                "uploader = widgets.FileUpload(accept='.wav,.mp3', multiple=False)\n",
                "tab2 = widgets.VBox([uploader])\n",
                "\n",
                "# Tab 3: Microphone Recording\n",
                "record_btn = widgets.Button(description=\"Start Recording (10s)\", button_style='danger')\n",
                "record_out = widgets.Output()\n",
                "def on_record_click(b):\n",
                "    with record_out:\n",
                "        record_btn.disabled = True\n",
                "        try:\n",
                "            record_audio(10)\n",
                "        except Exception as e:\n",
                "             print(f\"Error: {e}. (Note: Recording requires Colab/Browser context)\")\n",
                "        record_btn.disabled = False\n",
                "        \n",
                "record_btn.on_click(on_record_click)\n",
                "tab3 = widgets.VBox([record_btn, record_out])\n",
                "\n",
                "# Assemble Tabs\n",
                "tab.children = [tab1, tab2, tab3]\n",
                "tab.set_title(0, 'üéµ Presets')\n",
                "tab.set_title(1, 'üìÇ Upload')\n",
                "tab.set_title(2, 'üî¥ Record')\n",
                "display(tab)\n",
                "\n",
                "# Text Input Area\n",
                "text_input = widgets.Textarea(\n",
                "    value='Hello! This is a real-time voice cloning test. The quality is truly amazing.',\n",
                "    placeholder='Enter text to synthesize...',\n",
                "    description='Text:',\n",
                "    disabled=False,\n",
                "    layout=widgets.Layout(width='50%', height='100px')\n",
                ")\n",
                "\n",
                "# Post-Processing Options\n",
                "normalize_chk = widgets.Checkbox(value=False, description=\"Normalize Audio üéöÔ∏è\")\n",
                "\n",
                "clone_btn = widgets.Button(description=\"Clone Voice! üöÄ\", button_style='primary')\n",
                "out = widgets.Output()\n",
                "\n",
                "display(text_input, normalize_chk, clone_btn, out)\n",
                "\n",
                "# --- PROCESSING PIPELINE ---\n",
                "\n",
                "def run_cloning(b):\n",
                "    with out:\n",
                "        out.clear_output()\n",
                "        active_tab = tab.selected_index\n",
                "        input_path = None\n",
                "        \n",
                "        try:\n",
                "            # 1. Acquire Input Audio\n",
                "            if active_tab == 0: # Preset\n",
                "                 input_path = os.path.join(samples_dir, dropdown.value)\n",
                "                 print(f\"üéôÔ∏è Source: Preset ({dropdown.value})\")\n",
                "            \n",
                "            elif active_tab == 1: # Upload\n",
                "                 if not uploader.value:\n",
                "                     print(\"‚ùå User Error: Please upload a file first!\")\n",
                "                     return\n",
                "                 fname = list(uploader.value.keys())[0]\n",
                "                 content = uploader.value[fname]['content']\n",
                "                 input_path = \"uploaded_sample.wav\"\n",
                "                 with open(input_path, \"wb\") as f:\n",
                "                     f.write(content)\n",
                "                 print(f\"üéôÔ∏è Source: Upload ({fname})\")\n",
                "            \n",
                "            elif active_tab == 2: # Record\n",
                "                 if not os.path.exists(\"recording.wav\"):\n",
                "                     print(\"‚ùå User Error: Please record audio first!\")\n",
                "                     return\n",
                "                 input_path = \"recording.wav\"\n",
                "                 print(\"üéôÔ∏è Source: Microphone Recording\")\n",
                "            \n",
                "            # Start Timer\n",
                "            start_time = time.time()\n",
                "\n",
                "            # 2. Preprocess & Embed (Encoder)\n",
                "            print(\"‚è≥ Step 1/3: Preprocessing & Encoding Speaker Characteristics...\")\n",
                "            original_wav, sampling_rate = librosa.load(input_path)\n",
                "            preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n",
                "            embed = encoder.embed_utterance(preprocessed_wav)\n",
                "            print(\"   ‚úÖ Speaker embedding generated.\")\n",
                "\n",
                "            # 3. Synthesize Spectrogram (Synthesizer)\n",
                "            print(\"‚è≥ Step 2/3: Synthesizing Mel Spectrogram from Text...\")\n",
                "            specs = synthesizer.synthesize_spectrograms([text_input.value], [embed])\n",
                "            spec = specs[0]\n",
                "            print(\"   ‚úÖ Spectrogram generated.\")\n",
                "\n",
                "            # 4. Generate Waveform (Vocoder)\n",
                "            print(\"‚è≥ Step 3/3: Vocoding (Spectrogram -> Audio)...\")\n",
                "            generated_wav = vocoder.infer_waveform(spec)\n",
                "            \n",
                "            # 5. Post-Processing\n",
                "            if normalize_chk.value:\n",
                "                print(\"üéöÔ∏è Normalizing Audio Output...\")\n",
                "                generated_wav = librosa.util.normalize(generated_wav)\n",
                "            \n",
                "            # Stop Timer & Calculate RTF\n",
                "            end_time = time.time()\n",
                "            proc_time = end_time - start_time\n",
                "            duration = len(generated_wav) / synthesizer.sample_rate\n",
                "            rtf = proc_time / duration\n",
                "            print(f\"‚ö° Performance Analysis: Generated {duration:.2f}s of audio in {proc_time:.2f}s (RTF: {rtf:.3f}x)\")\n",
                "            \n",
                "            # 6. Output Audio & Download\n",
                "            print(\"üéâ Synthesis Complete! Playing Audio:\")\n",
                "            display(Audio(generated_wav, rate=synthesizer.sample_rate))\n",
                "            \n",
                "            # Generate Download Link (Base64)\n",
                "            buf = io.BytesIO()\n",
                "            sf.write(buf, generated_wav, synthesizer.sample_rate, format='WAV')\n",
                "            b64 = b64encode(buf.getvalue()).decode()\n",
                "            html_str = f'<a href=\"data:audio/wav;base64,{b64}\" download=\"cloned_voice.wav\" style=\"background-color:#4CAF50; color:white; padding:8px 16px; text-decoration:none; border-radius:4px;\">‚¨áÔ∏è Download Audio (WAV)</a>'\n",
                "            display(HTML(html_str))\n",
                "            \n",
                "            # 7. Visualization\n",
                "            print(\"\\nüìä Generating Scholarly Analysis...\")\n",
                "            visualize_results(original_wav, generated_wav, spec, embed)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Execution Error: {e}\")\n",
                "\n",
                "clone_btn.on_click(run_cloning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- OPTION B: MODERN GRADIO UI ---\n",
                "\n",
                "import gradio as gr\n",
                "\n",
                "def gradio_cloning_pipeline(text, audio_input, normalize):\n",
                "    if audio_input is None:\n",
                "        raise gr.Error(\"Please provide an input audio file (upload or record).\")\n",
                "        \n",
                "    # 1. Load Audio\n",
                "    # Gradio passes audio as (sample_rate, numpy_array) or filepath depending on type\n",
                "    # Here type=\"filepath\" is safest for consistency with librosa\n",
                "    original_wav, sampling_rate = librosa.load(audio_input)\n",
                "    \n",
                "    # 2. Encoder\n",
                "    preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n",
                "    embed = encoder.embed_utterance(preprocessed_wav)\n",
                "    \n",
                "    # 3. Synthesizer\n",
                "    specs = synthesizer.synthesize_spectrograms([text], [embed])\n",
                "    spec = specs[0]\n",
                "    \n",
                "    # 4. Vocoder\n",
                "    generated_wav = vocoder.infer_waveform(spec)\n",
                "    if normalize:\n",
                "        generated_wav = librosa.util.normalize(generated_wav)\n",
                "        \n",
                "    return (synthesizer.sample_rate, generated_wav)\n",
                "\n",
                "iface = gr.Interface(\n",
                "    fn=gradio_cloning_pipeline,\n",
                "    inputs=[\n",
                "        gr.Textbox(label=\"Text to Synthesize\", value=\"This is a test of the Deepfake Audio cloning system.\", lines=3),\n",
                "        gr.Audio(label=\"Reference Voice\", type=\"filepath\"), # Supports upload & mic\n",
                "        gr.Checkbox(label=\"Normalize Output\", value=False)\n",
                "    ],\n",
                "    outputs=gr.Audio(label=\"Cloned Voice\"),\n",
                "    title=\"üéôÔ∏è Deepfake Audio Studio\",\n",
                "    description=\"Clone any voice in seconds using the SV2TTS framework. Upload a sample, type your text, and generate!\",\n",
                "    theme=\"default\"\n",
                ")\n",
                "\n",
                "print(\"üöÄ Launching Gradio Interface...\")\n",
                "iface.launch(share=True, debug=True)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}