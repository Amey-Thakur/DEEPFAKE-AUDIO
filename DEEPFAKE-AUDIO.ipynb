{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<h1 align=\"center\">üéôÔ∏è Deepfake Audio</h1>\n",
                "<h3 align=\"center\"><i>A neural voice cloning studio powered by SV2TTS technology</i></h3>\n",
                "\n",
                "<div align=\"center\">\n",
                "\n",
                "| **Author** | **Profiles** |\n",
                "|:---:|:---|\n",
                "| **Amey Thakur** | [![GitHub](https://img.shields.io/badge/GitHub-Amey--Thakur-181717?logo=github)](https://github.com/Amey-Thakur) [![ORCID](https://img.shields.io/badge/ORCID-0000--0001--5644--1575-A6CE39?logo=orcid)](https://orcid.org/0000-0001-5644-1575) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Amey_Thakur-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=0inooPgAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Amey_Thakur-20BEFF?logo=kaggle)](https://www.kaggle.com/ameythakur20) |\n",
                "| **Mega Satish** | [![GitHub](https://img.shields.io/badge/GitHub-msatmod-181717?logo=github)](https://github.com/msatmod) [![ORCID](https://img.shields.io/badge/ORCID-0000--0002--1844--9557-A6CE39?logo=orcid)](https://orcid.org/0000-0002-1844-9557) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Mega_Satish-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=7Ajrr6EAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Mega_Satish-20BEFF?logo=kaggle)](https://www.kaggle.com/megasatish) |\n",
                "\n",
                "---\n",
                "\n",
                "**Attribution:** This project builds upon the foundational work of [CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning).\n",
                "\n",
                "üöÄ **Live Demo:** [Hugging Face Space](https://huggingface.co/spaces/ameythakur/Deepfake-Audio) | üé• **Video Demo:** [YouTube](https://youtu.be/i3wnBcbHDbs) | üíª **Repository:** [GitHub](https://github.com/Amey-Thakur/DEEPFAKE-AUDIO)\n",
                "\n",
                "<a href=\"https://youtu.be/i3wnBcbHDbs\">\n",
                "  <img src=\"https://img.youtube.com/vi/i3wnBcbHDbs/0.jpg\" alt=\"Video Demo\" width=\"60%\">\n",
                "</a>\n",
                "\n",
                "</div>\n",
                "\n",
                "## üìñ Introduction\n",
                "\n",
                "> **An audio deepfake is when a ‚Äúcloned‚Äù voice that is potentially indistinguishable from the real person‚Äôs is used to produce synthetic audio.**\n",
                "\n",
                "This research notebook demonstrates the **SV2TTS (Speaker Verification to Text-to-Speech)** framework, a three-stage deep learning pipeline capable of cloning a voice from a mere 5 seconds of audio.\n",
                "\n",
                "### The Pipeline Components\n",
                "1.  **Speaker Encoder**: A Recurrent Neural Network (RNN) that condenses the *timbre* and *prosody* of the reference audio into a fixed-length vector (embedding).\n",
                "2.  **Synthesizer**: A Tacotron-2 based implementation that takes text and the speaker embedding to generate a visual representation of speech (Mel Spectrogram).\n",
                "3.  **Vocoder**: A WaveRNN network that iteratively generates the raw audio waveform from the Mel Spectrogram, sample by sample."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚òÅÔ∏è Cloud Environment Setup\n",
                "Execute the following cell **only** if you are running this notebook in a cloud environment like **Google Colab** or **Kaggle**. \n",
                "\n",
                "This script will:\n",
                "1.  Clone the [DEEPFAKE-AUDIO repository](https://github.com/Amey-Thakur/DEEPFAKE-AUDIO).\n",
                "2.  Install system-level dependencies (e.g., `libsndfile1` for audio processing).\n",
                "3.  Install Python libraries required for signal processing and deep learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Detect Cloud Environment (Colab/Kaggle)\n",
                "try:\n",
                "    shell = get_ipython()\n",
                "    if 'google.colab' in str(shell):\n",
                "        print(\"üíª Detected Google Colab Environment. Initiating setup...\")\n",
                "        \n",
                "        # 1. Clone the Repository\n",
                "        if not os.path.exists(\"DEEPFAKE-AUDIO\"):\n",
                "            print(\"‚¨áÔ∏è Cloning DEEPFAKE-AUDIO repository...\")\n",
                "            shell.system(\"git clone https://github.com/Amey-Thakur/DEEPFAKE-AUDIO\")\n",
                "        \n",
                "        # 2. Change Working Directory\n",
                "        os.chdir(\"/content/DEEPFAKE-AUDIO\")\n",
                "        \n",
                "        # 3. Pull Latest Changes (Ensure freshness)\n",
                "        print(\"üîÑ Synchronizing with remote repository...\")\n",
                "        shell.system(\"git pull\")\n",
                "        \n",
                "        # 4. Install System Dependencies\n",
                "        # libsndfile1 is crucial for reading/writing audio files via SoundFile/Librosa\n",
                "        print(\"üîß Installing system dependencies (libsndfile1)...\")\n",
                "        shell.system(\"apt-get install -y libsndfile1\")\n",
                "        \n",
                "        # 5. Install Python Dependencies\n",
                "        print(\"üì¶ Installing Python libraries...\")\n",
                "        shell.system(\"pip install librosa==0.9.2 unidecode webrtcvad inflect umap-learn scikit-learn>=1.3 tqdm scipy matplotlib>=3.7 Pillow>=10.2 soundfile huggingface_hub\")\n",
                "        \n",
                "        print(\"‚úÖ Environment setup complete. Ready for cloning.\")\n",
                "    else:\n",
                "        print(\"üè† Running in local or custom environment. Skipping cloud setup.\")\n",
                "except NameError:\n",
                "    print(\"üè† Running in local or custom environment. Skipping cloud setup.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Model & Data Initialization\n",
                "\n",
                "We prioritize data availability to ensure the notebook runs smoothly regardless of the platform. The system checks for checkpoints in this order:\n",
                "\n",
                "1.  **Repository Local** (`Dataset/`): Fast local access if cloned.\n",
                "2.  **Kaggle Dataset** (`/kaggle/input/deepfakeaudio/`): Pre-loaded environment data.\n",
                "    *   *Reference*: [Amey Thakur's Kaggle Dataset](https://www.kaggle.com/datasets/ameythakur20/deepfakeaudio)\n",
                "    *   *Kaggle Profile*: [ameythakur20](https://www.kaggle.com/ameythakur20)\n",
                "3.  **HuggingFace Auto-Download**: Robust fallback for fresh environments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "import zipfile\n",
                "import shutil\n",
                "\n",
                "# Register 'Source Code' to Python path for module imports\n",
                "source_path = os.path.abspath(\"Source Code\")\n",
                "if source_path not in sys.path:\n",
                "    sys.path.append(source_path)\n",
                "\n",
                "print(f\"üìÇ Working Directory: {os.getcwd()}\")\n",
                "print(f\"‚úÖ Module Path Registered: {source_path}\")\n",
                "\n",
                "# Define paths for model checkpoints\n",
                "extract_path = \"pretrained_models\"\n",
                "zip_path = \"Dataset/pretrained.zip\"\n",
                "\n",
                "if not os.path.exists(extract_path):\n",
                "    os.makedirs(extract_path)\n",
                "\n",
                "# --- üß† Checkpoint Verification Strategy ---\n",
                "print(\"‚¨áÔ∏è Verifying Model Availability...\")\n",
                "\n",
                "# Priority 1: Check Local Repository 'Dataset/' folder\n",
                "core_models = [\"encoder.pt\", \"synthesizer.pt\", \"vocoder.pt\"]\n",
                "dataset_models_present = all([os.path.exists(os.path.join(\"Dataset\", m)) for m in core_models])\n",
                "\n",
                "if dataset_models_present:\n",
                "     print(\"‚úÖ Found high-priority local models in 'Dataset/'. verified.\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Models not found in 'Dataset/'. Attempting fallback strategies...\")\n",
                "    \n",
                "    # Priority 3 (Fallback): Auto-download from HuggingFace via utils script\n",
                "    try:\n",
                "        from utils.default_models import ensure_default_models\n",
                "        ensure_default_models(Path(\"pretrained_models\"))\n",
                "        print(\"‚úÖ Models successfully acquired via HuggingFace fallback.\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Critical: Could not auto-download models. Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Architecture Loading\n",
                "\n",
                "We now initialize the three distinct neural networks that comprise the SV2TTS framework. Please ensure you are running on a **GPU Runtime** (e.g., T4 on Colab) for optimal performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from encoder import inference as encoder\n",
                "from synthesizer.inference import Synthesizer\n",
                "from vocoder import inference as vocoder\n",
                "import numpy as np\n",
                "import torch\n",
                "from pathlib import Path\n",
                "\n",
                "# Hardware Acceleration Check\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"üéØ Computation Device: {device}\")\n",
                "\n",
                "def resolve_checkpoint(component_name, legacy_path_suffix):\n",
                "    \"\"\"\n",
                "    Intelligently resolves the path to model checkpoints based on priority.\n",
                "    1. Repository /Dataset/ folder.\n",
                "    2. Kaggle Input directory.\n",
                "    3. Auto-downloaded 'pretrained_models'.\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Priority: Repository Local (Dataset/)\n",
                "    dataset_p = Path(\"Dataset\") / f\"{component_name.lower()}.pt\"\n",
                "    if dataset_p.exists():\n",
                "        print(f\"üü¢ Loading {component_name} from Repository: {dataset_p}\")\n",
                "        return dataset_p\n",
                "\n",
                "    # 2. Priority: Kaggle Environment\n",
                "    kaggle_p = Path(\"/kaggle/input/deepfakeaudio\") / f\"{component_name.lower()}.pt\"\n",
                "    if kaggle_p.exists():\n",
                "        print(f\"üü¢ Loading {component_name} from Kaggle Input: {kaggle_p}\")\n",
                "        return kaggle_p\n",
                "    \n",
                "    # 3. Priority: Auto-Downloaded Fallback\n",
                "    default_p = Path(\"pretrained_models/default\") / f\"{component_name.lower()}.pt\"\n",
                "    if default_p.exists():\n",
                "        print(f\"üü¢ Loading {component_name} from Auto-Download: {default_p}\")\n",
                "        return default_p\n",
                "\n",
                "    # 4. Legacy/Manual Paths\n",
                "    legacy_p = Path(\"pretrained_models\") / legacy_path_suffix\n",
                "    if legacy_p.exists():\n",
                "         if legacy_p.is_dir():\n",
                "             pts = [f for f in legacy_p.glob(\"*.pt\") if f.is_file()]\n",
                "             if pts: return pts[0]\n",
                "             pts_rec = [f for f in legacy_p.rglob(\"*.pt\") if f.is_file()]\n",
                "             if pts_rec: return pts_rec[0]\n",
                "         return legacy_p\n",
                "            \n",
                "    print(f'‚ö†Ô∏è Warning: Checkpoint for {component_name} not found!')\n",
                "    return None\n",
                "\n",
                "print(\"‚è≥ Initializing Neural Networks...\")\n",
                "\n",
                "try:\n",
                "    # 1. Encoder: Visualizes the voice's unique characteristics\n",
                "    encoder_path = resolve_checkpoint(\"Encoder\", \"encoder/saved_models\")\n",
                "    encoder.load_model(encoder_path)\n",
                "\n",
                "    # 2. Synthesizer: Generates spectrograms from text\n",
                "    synth_path = resolve_checkpoint(\"Synthesizer\", \"synthesizer/saved_models/logs-pretrained/taco_pretrained\")\n",
                "    synthesizer = Synthesizer(synth_path)\n",
                "\n",
                "    # 3. Vocoder: Converts spectrograms to audio waveforms\n",
                "    vocoder_path = resolve_checkpoint(\"Vocoder\", \"vocoder/saved_models/pretrained\")\n",
                "    vocoder.load_model(vocoder_path)\n",
                "\n",
                "    print(\"‚úÖ All models loaded successfully. The pipeline is ready.\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Initialization Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Inference Interface\n",
                "\n",
                "Experience the logic through our simple **Neural Voice Cloning Studio**. Type your text, select a reference voice, and witness the magic of AI."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ipywidgets as widgets\n",
                "from IPython.display import display, Audio, clear_output\n",
                "import librosa\n",
                "import numpy as np\n",
                "import time\n",
                "import os\n",
                "\n",
                "# --- üìÇ Configuration ---\n",
                "sample_roots = [\n",
                "    \"Source Code/samples\",\n",
                "    \"Dataset/samples\",\n",
                "    \"d:/GitHub/DEEPFAKE-AUDIO/Source Code/samples\",\n",
                "    \"d:/GitHub/DEEPFAKE-AUDIO/Dataset/samples\",\n",
                "    \"/kaggle/input/deepfakeaudio/samples\"\n",
                "]\n",
                "\n",
                "samples_dir = None\n",
                "for d in sample_roots:\n",
                "    if os.path.exists(d):\n",
                "        files = [f for f in os.listdir(d) if f.endswith((\".wav\", \".mp3\"))]\n",
                "        if len(files) > 0:\n",
                "            samples_dir = d\n",
                "            break\n",
                "\n",
                "presets = {}\n",
                "if samples_dir:\n",
                "    files = sorted([f for f in os.listdir(samples_dir) if f.endswith((\".wav\", \".mp3\"))])\n",
                "    for f in files:\n",
                "        presets[os.path.splitext(f)[0]] = os.path.join(samples_dir, f)\n",
                "\n",
                "# --- üíÖ Interface Components ---\n",
                "style = {'description_width': 'initial'}\n",
                "\n",
                "preset_dropdown = widgets.Dropdown(\n",
                "    options=[('Custom Upload', None)] + sorted(list(presets.items())),\n",
                "    value=presets.get(\"Donald Trump\", None) if \"Donald Trump\" in presets else None,\n",
                "    description='Reference Voice:',\n",
                "    style=style\n",
                ")\n",
                "\n",
                "upload_widget = widgets.FileUpload(\n",
                "    accept='.wav,.mp3',\n",
                "    multiple=False,\n",
                "    description='Upload Voice:',\n",
                "    style=style\n",
                ")\n",
                "\n",
                "text_area = widgets.Textarea(\n",
                "    value='Welcome to Deepfake Audio. This is a simple neural voice cloning demonstration.',\n",
                "    placeholder='Enter text to synthesize...',\n",
                "    description='Target Script:',\n",
                "    layout={'width': '100%', 'height': '100px'},\n",
                "    style=style\n",
                ")\n",
                "\n",
                "generate_btn = widgets.Button(\n",
                "    description='üöÄ Generate Voice Clone',\n",
                "    button_style='primary',\n",
                "    layout={'width': '100%', 'height': '50px'}\n",
                ")\n",
                "\n",
                "output_area = widgets.Output()\n",
                "\n",
                "# --- üß† Logic ---\n",
                "def run_synthesis(_):\n",
                "    with output_area:\n",
                "        clear_output()\n",
                "        print(\"‚è≥ Processing...\")\n",
                "        \n",
                "        try:\n",
                "            # Get Reference Audio\n",
                "            if upload_widget.value:\n",
                "                uploaded_file = list(upload_widget.value.values())[0]\n",
                "                # Save temp\n",
                "                with open(\"temp_ref.wav\", \"wb\") as f:\n",
                "                    f.write(uploaded_file['content'])\n",
                "                ref_path = \"temp_ref.wav\"\n",
                "            else:\n",
                "                ref_path = preset_dropdown.value\n",
                "            \n",
                "            if not ref_path:\n",
                "                print(\"‚ùå Error: Please select a preset or upload a custom voice.\")\n",
                "                return\n",
                "\n",
                "            script = text_area.value.strip()\n",
                "            if not script:\n",
                "                print(\"‚ùå Error: Please enter a target script.\")\n",
                "                return\n",
                "\n",
                "            # 1. Encode Voice\n",
                "            wav, sr = librosa.load(ref_path, sr=None)\n",
                "            preprocessed_wav = encoder.preprocess_wav(wav, sr)\n",
                "            embed = encoder.embed_utterance(preprocessed_wav)\n",
                "            \n",
                "            # 2. Synthesize Spectrogram\n",
                "            specs = synthesizer.synthesize_spectrograms([script], [embed])\n",
                "            \n",
                "            # 3. Vocode Waveform\n",
                "            generated_wav = vocoder.infer_waveform(specs[0])\n",
                "            \n",
                "            # Post-Process\n",
                "            generated_wav = librosa.util.normalize(generated_wav) * 0.98\n",
                "            \n",
                "            clear_output()\n",
                "            print(\"‚úÖ Synthesis Complete!\")\n",
                "            display(Audio(generated_wav, rate=synthesizer.sample_rate, autoplay=True))\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Error: {e}\")\n",
                "\n",
                "generate_btn.on_click(run_synthesis)\n",
                "\n",
                "# --- Layout ---\n",
                "display(widgets.VBox([\n",
                "    widgets.HTML(\"<h2>üéôÔ∏è Neural Voice Cloning Studio</h2>\"),\n",
                "    widgets.HBox([preset_dropdown, upload_widget]),\n",
                "    text_area,\n",
                "    generate_btn,\n",
                "    output_area\n",
                "]))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}