"""
Deepfake Audio - Encoder Training Script
----------------------------------------
This script governs the training process for the Speaker Encoder model.
It relies on the preprocessed mel spectrograms generated by `encoder_preprocess.py`.

The Speaker Encoder is trained to optimize the Generalized End-to-End (GE2E) loss,
learning to map diverse utterances from the same speaker to a compact latent vector (d-vector).

Authors:
    - Amey Thakur (https://github.com/Amey-Thakur)
    - Mega Satish (https://github.com/msatmod)

Repository:
    - https://github.com/Amey-Thakur/DEEPFAKE-AUDIO

Release Date:
    - February 06, 2021

License:
    - MIT License

Description:
    The training loop performs the following operations:
    1.  **Data Loading**: Batches preprocessed mel spectrograms.
    2.  **Forward Pass**: Computes embeddings via the LSTM-based encoder.
    3.  **Loss Calculation**: Computes the GE2E loss to maximize similarity between same-speaker embeddings
        and minimize similarity between different speakers.
    4.  **Backpropagation**: Updates model weights (requires a GPU).
    5.  **Visualization**: Optionally logs training metrics to a Visdom server.
"""

import argparse
from pathlib import Path
from typing import Dict, Any

# Standard Library Imports
from utils.argutils import print_args

# Internal Training Routine
from encoder.train import train


def main() -> None:
    """
    Main execution routine for encoder training.
    
    Orchestrates the training lifecycle, handling argument parsing,
    directory creation, and delegating to the core training loop.
    """
    
    parser = argparse.ArgumentParser(
        description="Trains the speaker encoder. Requires `encoder_preprocess.py` artifacts.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # -------------------------------------------------------------------------
    # Positional Arguments
    # -------------------------------------------------------------------------
    parser.add_argument("run_id", type=str, help= \
        "Unique Name/ID for this training run. If a run with this ID exists, "
        "training will resume from the last checkpoint unless -f is used.")
        
    parser.add_argument("clean_data_root", type=Path, help= \
        "Path to the directory containing the preprocessed data (output of "
        "encoder_preprocess.py). Typically: <datasets_root>/SV2TTS/encoder/.")
    
    # -------------------------------------------------------------------------
    # Optional Arguments (Model IO & Logging)
    # -------------------------------------------------------------------------
    parser.add_argument("-m", "--models_dir", type=Path, default="encoder/saved_models/", help=\
        "Directory where model checkpoints, backups, and training plots will be saved.")
        
    parser.add_argument("-v", "--vis_every", type=int, default=10, help= \
        "Step interval for updating training loss plots and metrics.")
        
    parser.add_argument("-u", "--umap_every", type=int, default=100, help= \
        "Step interval for computing UMAP projections of embeddings "
        "(visualizes cluster separation). Set 0 to disable.")
        
    parser.add_argument("-s", "--save_every", type=int, default=500, help= \
        "Step interval for saving model checkpoints to disk. Set 0 to disable.")
        
    parser.add_argument("-b", "--backup_every", type=int, default=7500, help= \
        "Step interval for maintaining backup checkpoints. Set 0 to disable.")
        
    parser.add_argument("-f", "--force_restart", action="store_true", help= \
        "If set, overwrites any existing checkpoints for this run_id and starts from scratch.")
        
    parser.add_argument("--visdom_server", type=str, default="http://localhost", help= \
        "URL of the Visdom server for real-time visualization.")
        
    parser.add_argument("--no_visdom", action="store_true", help= \
        "Disable Visdom logging entirely.")
        
    args = parser.parse_args()
    
    # =========================================================================
    # Setup & Execution
    # =========================================================================
    # Ensure model output directory exists
    args.models_dir.mkdir(exist_ok=True, parents=True)
    
    # Log configuration parameters
    print_args(args, parser)
    
    # Invoke the training loop
    # We unpack args as a dictionary to match the function signature of `train`
    train(**vars(args))


if __name__ == "__main__":
    main()
    